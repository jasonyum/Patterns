{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting a Million Integer Array\n",
    "Given an array of a million integers between zero and a billion, out of order, how can you efficiently sort it? Assume that you cannot store an array of a billion elements in memory.\n",
    "\n",
    "Idea: Sorting by an algorithm like quicksort or merge sort would give us an average time complexity of O(N log N). But we can take advantage of the fact that our input is bounded and only consists of integers to do even better. One algorithm that can perform particularly well in these cases is called radix sort.\n",
    "\n",
    "Each of these sorts is performed using counting sort, which gets around the efficiency limits of comparison sorts like quicksort. In counting sort, we bucket each number into an array of size equal to our base, in this case 10. Then, we read off the elements in each bucket, in order, to get the new sorted array.\n",
    "\n",
    "From Wikipedia:\n",
    "\n",
    "> The topic of the efficiency of radix sort compared to other sorting algorithms is somewhat tricky and subject to quite a lot of misunderstandings. Whether radix sort is equally efficient, less efficient or more efficient than the best comparison-based algorithms depends on the details of the assumptions made. Radix sort complexity is O(wn) for n keys which are integers of word size w. Sometimes w is presented as a constant, which would make radix sort better (for sufficiently large n) than the best comparison-based sorting algorithms, which all perform Θ(n log n) comparisons to sort n keys.\n",
    "\n",
    ">The counter argument is that comparison-based algorithms are measured in number of comparisons, not actual time complexity. Under some assumptions the comparisons will be constant time on average, under others they will not. Comparisons of randomly generated keys takes constant time on average, as keys differ on the very first bit in half the cases, and differ on the second bit in half of the remaining half, and so on, resulting in an average of two bits that need to be compared. In a sorting algorithm the first comparisons made satisfies the randomness condition, but as the sort progresses the keys compared are clearly not randomly chosen anymore. For example, consider a bottom-up merge sort. The first pass will compare pairs of random keys, but the last pass will compare keys that are very close in the sorting order. This makes merge sort, on this class of inputs, take Ω(n (log n)2) time. That assumes all memory accesses cost the same, which is not a physically reasonable assumption as we scale n to infinity, and not, in practice, how real computers work. This argument extends from the observation that computers are filled with different types of memory (cache, system memory, virtual memory etc.) in different and limited quantities. Modern operating systems will position variables into these different systems automatically making memory access time differ widely as more and more memory is utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counting_sort(array, digit, base=10):\n",
    "    counts = [[] for _ in range(base)]\n",
    "    \n",
    "    for num in array: \n",
    "        d = (num // base ** digit) % base\n",
    "        counts[d].append(num) \n",
    "    \n",
    "    result = [] \n",
    "    for bucket in counts: \n",
    "        result.extend(bucket)\n",
    "        \n",
    "    return result\n",
    "\n",
    "def radix_sort(array, digits=10): \n",
    "    for digit in range(digits): \n",
    "        array = counting_sort(array,digit) \n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting sort takes O(N + M), where M is the maximum bucket size. For our problem, we can consider this to be O(N). We must perform this sort k times, where k is the number of digits in the maximum number, in this case 10. The time complexity of this algorithm is therefore O(10 * N), or if we treat k as constant, O(N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
